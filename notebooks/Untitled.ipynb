{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/54116/demo/data/raw/Airbnb.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6285e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95590a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación de variables categóricas\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa243f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ee38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da573133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\54116\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\54116\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\54116\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "# Normalización \n",
    "scaler = StandardScaler()\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c3ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nor\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae20d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el DataFrame limpio\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "selected_cols = ['price', 'accommodates', 'bedrooms', 'bathrooms', 'review_scores_rating', 'number_of_reviews', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value']\n",
    "correlation_matrix = df[selected_cols].corr(numeric_only=True)\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(y=df[col], order=df[col].value_counts().index)\n",
    "    plt.title(f'Count of Categories in {col}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51131197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relación entre precio y número de habitaciones\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='bedrooms', y='price')\n",
    "plt.title('Relación entre Precio y Número de Habitaciones')\n",
    "plt.xlabel('Número de Habitaciones')\n",
    "plt.ylabel('Precio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1cb956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Tamaño del dataset después de eliminar valores nulos: (3818, 92)\n",
      "Mejores hiperparámetros: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Mean Squared Error: 4081.735090975775\n",
      "R^2 Score: 0.497094412825582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/54116/demo/data/raw/Airbnb.xlsx\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Verificar el tamaño del dataset \n",
    "print(f\"Tamaño del dataset después de eliminar valores nulos: {df.shape}\")\n",
    "\n",
    "# Seleccionar características (X) y variable objetivo (y)\n",
    "X = df[['number_of_reviews', 'accommodates', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Verificar si hay suficientes datos para dividir\n",
    "if len(df) > 1:\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Crear el modelo de árbol de decisión\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    # Definir los parámetros para GridSearchCV\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Realizar la búsqueda de hiperparámetros\n",
    "    try:\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, error_score='raise')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Mejor modelo encontrado por GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Hacer predicciones\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "        print(f\"R^2 Score: {r2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training or evaluation: {e}\")\n",
    "else:\n",
    "    print(\"No hay suficientes datos para dividir en conjuntos de entrenamiento y prueba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee914f8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de34099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Tamaño del dataset después de eliminar valores nulos: (3818, 92)\n",
      "Mejores hiperparámetros: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Mean Squared Error: 3727.621631100263\n",
      "R^2 Score: 0.540724299012656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_excel(\"C:/Users/54116/demo/data/raw/Airbnb.xlsx\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Verificar el tamaño del dataset después de eliminar valores nulos\n",
    "print(f\"Tamaño del dataset después de eliminar valores nulos: {df.shape}\")\n",
    "\n",
    "# Seleccionar características (X) y variable objetivo (y)\n",
    "X = df[['number_of_reviews', 'accommodates', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Verificar si hay suficientes datos para dividir\n",
    "if len(df) > 1:\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Crear el modelo de Random Forest\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Definir los parámetros para GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Realizar la búsqueda de hiperparámetros\n",
    "    try:\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, error_score='raise')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Mejor modelo encontrado por GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Hacer predicciones\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "        print(f\"R^2 Score: {r2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training or evaluation: {e}\")\n",
    "else:\n",
    "    print(\"No hay suficientes datos para dividir en conjuntos de entrenamiento y prueba.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46124f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Tamaño del dataset después de eliminar valores nulos: (3818, 92)\n",
      "Mejores hiperparámetros: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Mean Squared Error: 3727.621631100263\n",
      "R^2 Score: 0.540724299012656\n",
      "Cross-Validated R^2 Score: 0.4773455183275418 ± 0.11665612839501888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_excel(\"C:/Users/54116/demo/data/raw/Airbnb.xlsx\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "\n",
    "print(f\"Tamaño del dataset después de eliminar valores nulos: {df.shape}\")\n",
    "\n",
    "# Seleccionar características (X) y variable objetivo (y)\n",
    "X = df[['number_of_reviews', 'accommodates', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Verificar si hay suficientes datos para dividir\n",
    "if len(df) > 1:\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Crear el modelo de Random Forest\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Definir los parámetros para GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Realizar la búsqueda de hiperparámetros\n",
    "    try:\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, error_score='raise')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Mejor modelo encontrado por GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Evaluar el modelo con cross-validation\n",
    "        cv_scores = cross_val_score(best_model, X_scaled, y, cv=5, scoring='r2')\n",
    "        mean_cv_score = np.mean(cv_scores)\n",
    "        std_cv_score = np.std(cv_scores)\n",
    "\n",
    "        # Hacer predicciones\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "        print(f\"Mean Squared Error: {mse}\")\n",
    "        print(f\"R^2 Score: {r2}\")\n",
    "        print(f\"Cross-Validated R^2 Score: {mean_cv_score} ± {std_cv_score}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training or evaluation: {e}\")\n",
    "else:\n",
    "    print(\"No hay suficientes datos para dividir en conjuntos de entrenamiento y prueba.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9513955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Tamaño del dataset después de eliminar valores nulos en 'price': (3818, 92)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 3054, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 125.346758\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 3054, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 124.923707\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 165\n",
      "[LightGBM] [Info] Number of data points in the train set: 3054, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 130.171906\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 165\n",
      "[LightGBM] [Info] Number of data points in the train set: 3055, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 131.229133\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 164\n",
      "[LightGBM] [Info] Number of data points in the train set: 3055, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 128.208183\n",
      "RandomForest - Cross-Validated R^2 Score: 0.34568925397031397 ± 0.20353820556168728\n",
      "GradientBoosting - Cross-Validated R^2 Score: 0.4515357260537695 ± 0.12985158184813703\n",
      "XGBoost - Cross-Validated R^2 Score: 0.29601125717163085 ± 0.22887877809685855\n",
      "LightGBM - Cross-Validated R^2 Score: 0.43542350056802404 ± 0.13139391014892118\n",
      "Mejor modelo: GradientBoosting\n",
      "Mean Squared Error: 3769.992092338896\n",
      "R^2 Score: 0.5355038863172863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_excel(\"C:/Users/54116/demo/data/raw/Airbnb.xlsx\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "\n",
    "print(f\"Tamaño del dataset después de eliminar valores nulos en 'price': {df.shape}\")\n",
    "\n",
    "# Seleccionar características (X) y variable objetivo (y)\n",
    "X = df[['number_of_reviews', 'accommodates', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']\n",
    "\n",
    "# Imputar valores faltantes en X\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Verificar si hay suficientes datos para dividir\n",
    "if len(df) > 1:\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Definir los modelos a comparar\n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(random_state=42),\n",
    "        'LightGBM': lgb.LGBMRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Evaluar cada modelo con cross-validation\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "        results[name] = {\n",
    "            'mean_cv_score': np.mean(cv_scores),\n",
    "            'std_cv_score': np.std(cv_scores)\n",
    "        }\n",
    "\n",
    "    # Imprimir los resultados\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name} - Cross-Validated R^2 Score: {result['mean_cv_score']} ± {result['std_cv_score']}\")\n",
    "\n",
    "    # Seleccionar el mejor modelo basado en cross-validated R^2 score\n",
    "    best_model_name = max(results, key=lambda k: results[k]['mean_cv_score'])\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "    # Entrenar el mejor modelo en el conjunto de entrenamiento\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Hacer predicciones\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluar el mejor modelo\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mejor modelo: {best_model_name}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R^2 Score: {r2}\")\n",
    "else:\n",
    "    print(\"No hay suficientes datos para dividir en conjuntos de entrenamiento y prueba.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c93de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "Best cross-validated R^2 score: 0.5070984340249302\n",
      "Mean Squared Error: 3755.171271031842\n",
      "R^2 Score: 0.5373299415794982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for GradientBoostingRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='r2',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validated R^2 score: {best_score}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24fb5f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model R^2 Score: 0.5373299415794982\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_model, 'best_gradient_boosting_model.pkl')\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = joblib.load('best_gradient_boosting_model.pkl')\n",
    "\n",
    "# Verify that the loaded model works as expected\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "print(f\"Loaded model R^2 Score: {r2_score(y_test, y_pred_loaded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461cc836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
